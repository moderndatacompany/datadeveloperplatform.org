{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#data-developer-platform","title":"Data Developer Platform","text":"<p>A Data Platform Specification, open for adoption by any data platform developer.</p>"},{"location":"#a-modern-way-to-run-data-engineering-teams","title":"A modern way to run data engineering teams","text":"<p>Data teams are drained from continuously plumbing integrations and fragile pipelines, which leaves little to no time to focus on the real deal - data and data applications. Businesses that have a good grasp on data realise that today data makes the difference between winning and losing the competitive edge. Many data-first organizations understood this long back and dedicated major projects to become data-first.</p> <p>The likes of Uber, Google, and Airbnb have shown an inspiring discipline around data. But what does data-first mean?\u00a0As the name suggests, it is putting data and data-powered decisions first while de-prioritising everything else, either through abstractions or delegation to intelligent design architectures. However, replicating data-first organisations that took years to develop their platforms customised to their own data stacks is not the solution since their stacks were and are catered to their specific internal architectures.</p> <p>A data-first stack is only truly data-first if it is built in accordance with your internal infrastructure. Therefore, we have the Data Developer Platform infrastructure specification, established and rolled out specifically for the data domain with strong inspiration from the well-proven and acclaimed\u00a0Internal Developer Platform\u00a0spec. We have tried to mirror the site structure as much as possible so readers can easily correlate the fundamentals.\u200d</p> <p>What is DDP?</p> <p>What is IDP?</p>"},{"location":"#who-should-read-the-data-developer-platform-site","title":"Who should read the Data Developer Platform site?","text":"<p>The data developer platform specification has been made with the ideology \"of the people, by the people, for the people.\" In other words, it is made by data developers and engineers for data developers and engineers and undoubtedly belongs to the lot. It is entirely open for development and improvement with the aid and influence of fresh technology.\u200d</p>"},{"location":"#how-to-contribute-to-the-data-developer-platform","title":"How to contribute to the Data Developer Platform?","text":"<p>We are always looking to improve and innovate the specification by taking into account valuable suggestions and inputs from the community. For any recommendations, feel free to reach out to us at info@tmdc.io</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#architecture","title":"Architecture","text":""},{"location":"architecture/#conceptual-architecture-the-trifecta","title":"Conceptual Architecture: The Trifecta","text":"<p>DDP allow users to operationalise the platform through a triple-plane conceptual architecture where the control is forked between one central plane for core universal components, one or more development planes for workload management, and one or more data activation planes for localised or domain-specific deployment.</p> <p>Control &amp; Data plane separation decouples the governance and execution of data applications. This gives the flexibility to run the DDP in a hybrid environment and deploy a multi-tenant data architecture. Organizations can manage cloud storage and compute instances with a centralised control plane.</p> <p></p>"},{"location":"architecture/#control-plane","title":"Control Plane","text":"<p>The Control Plane helps admins govern the data ecosystem through centralised management and control of vertical components.</p> <ul> <li>Policy-based and purpose-driven access control of various touchpoints in cloud-native environments, with precedence to local ownership.</li> <li>Orchestration of data workloads, compute cluster life-cycle management, and version control of a Data Operating System\u2019s resources.</li> <li>Metadata management of different types of data assets.</li> </ul>"},{"location":"architecture/#development-plane","title":"Development Plane","text":"<p>The Development plane helps data developers, specifically data engineers, to create workload specifications for data products and data applications. </p> <ul> <li>Single specification file to manage end-to-end workloads for data products and applications</li> <li>Resource hub available for elastic provisioning</li> <li>Declarative environment and application configuration management</li> <li>Automated and version-controlled deployment</li> </ul>"},{"location":"architecture/#data-activation-plane","title":"Data Activation Plane","text":"<p>The Data Plane helps data developers to deploy, manage and scale\u00a0data products.</p> <ul> <li>Federated SQL query engine.</li> <li>Declarative stack to ingest, process, and extensively syndicate data.</li> <li>Complex event processing engine for stateful computations over data streams.</li> <li>Declarative DevOps SDK to publish data products and apps in production.</li> </ul>"},{"location":"architecture/#integrating-the-planes","title":"Integrating the Planes","text":""},{"location":"architecture/#integrating-control-and-development-planes","title":"Integrating Control and Development Planes","text":"<p>The control plane takes the lead on universal jobs, such as metadata management, governance, and orchestration. The orchestrator in the control plane consumes instructions from the specification file in the development plane and triggers the infrastructure by provisioning and de-provisioning resources and executing the required runs. For every trigger, the orchestrator checks in with the governance engine to validate policy execution and logs metadata with the metadata engine.</p>"},{"location":"architecture/#integrating-development-and-data-activation-planes","title":"Integrating Development and Data Activation Planes","text":"<p>The specification generated in the development plane is deployed and run on the data activation plane. While the resources are arranged in the development plane, they are executed in the activation plane, where the actual operations on data take place. Ideally, a single specification file in the development plane has a 1:1 relationship with a data product in the data activation plane, but this is not necessarily the norm. Developers can also build and trigger a config file with multiple data product references to target a specific use case.</p>"},{"location":"architecture/#integrating-control-and-data-activation-planes","title":"Integrating Control and Data Activation Planes","text":"<p>The orchestrator in the control plane is the heart of the DDP and ensures the resources in the specification (from the development plane) are executed in the right order in the data activation plane. For every operation on data, the orchestrator checks in with the governance engine to validate access and masking policies on the data. Every operation also generates tons of metadata which get logged into a database through the metadata engine.</p>"},{"location":"architecture/#technical-architecture-multilayered-kernel","title":"Technical Architecture: Multilayered Kernel","text":"<p>DDP is enabled through a multilayered kernel architecture that allows dedicated platform engineering teams to operate the system\u2019s core primitives without affecting the business or application user\u2019s day-to-day operations. Like the kernel of any operating system, it facilitates communications between users and primitives of the data operating system on the one hand and the binary world of the machines on the other. Each layer is an abstraction that translates low level-APIs into high level-APIs for usage by the different layers, components, or users of a Data Operating System. The kernels can logically be separated into three layers.</p> <p>The layered architecture promotes a unified experience as opposed to the complexity overheads of a microservices architecture imposed by the modern data stack. While the microservices architecture has some benefits, it also comes with performance, maintenance, security, and expertise overheads that can ultimately cost the organisation low ROI on data teams and high time to ROI for data applications. On the other hand, the layered approach promotes a loosely coupled yet tightly integrated set of components to disband the disadvantages of the modern data stack.</p>"},{"location":"architecture/#cloud-kernel","title":"Cloud Kernel","text":"<p>Cloud Kernel makes it possible for a Data Operating System to work with multiple cloud platforms without requiring specific integrations for each one. The Data Operating System uses several custom-built operators to abstract the VMs and network systems provisioned by the cloud provider. This allows users to not worry about the underlying protocols of the cloud provider and only communicate with the high-level APIs provided by the cloud kernel, making DDP truly cloud-agnostic.</p>"},{"location":"architecture/#core-kernel","title":"Core Kernel","text":"<p>Core Kernel provides another degree of abstraction by further translating the APIs of the cloud kernel into higher-order functions. From a user\u2019s perspective, the core kernel is where the activities like resource allocation, orchestration of primitives, scheduling, and database management occur. The cluster and compute that you need to carry out processes are declared here; the core kernel then communicates with the cloud kernel on your behalf to provision the requisite VMs or node pools and pods.</p>"},{"location":"architecture/#user-kernel","title":"User Kernel","text":"<p>User kernel is the third layer of abstraction over the APIs of the core and cloud kernels. The secondary extension points and programming paradigms of the Data Operating System, like the various Stacks and certain Primitives, can be envisioned to be working at this level. While you can directly communicate with the cloud kernel APIs, as a user of the Data Operating System, you can choose to rather work with the core and user kernels alone. The core kernel is where users work with essential features like Security, Metadata Management, and Resource Orchestration. In contrast, the user kernel can be thought of as the layer where the users have complete flexibility in terms of which components or primitives they want to leverage and which they do not require.</p> <p></p>"},{"location":"architecture/#structural-architecture-hierarchical-model","title":"Structural Architecture: Hierarchical Model","text":"<p>\u201cComplex subsystems can evolve from simple systems only if there are stable intermediate forms. That may explain why hierarchies are so common in the systems nature presents to us among all possible complex forms. They reduce the amount of information that any part of the system has to keep track of.\u201d ~ Donella H. Meadows, author of Thinking in Systems** </p> <p>Even Amazon identified this necessary hierarchical pivot right before AWS became the Operating System for Developers to build applications. We can get a clear picture of that from a published interview with one of the core members of AWS.</p> <p>\u201cIf you believe developers will build applications from scratch using web services as\u00a0primitive building blocks, then the operating system becomes the Internet,\u201d says Jassy (AWS Leader and SVP) \u2014 an approach to development that had not yet been considered. Amazon asked itself what the\u00a0key components of that operating system would be, what already existed, and what Amazon could contribute.\u201d ~ Andy Jassy, SVP and Leader @AWS </p> <p>How about considering a pivot for the Data Landscape in the same spirit?</p> <ul> <li>The Hierarchical Infrastructure design is the fundamental need for scalable systems.</li> <li>A composable set of resources - fundamental atomic building blocks and the means to quick pivots into variant higher-order frameworks or data design architectures such as meshes and fabrics.</li> <li>A fundamental systems thinking approach to reap value from frequent and disruptive innovations without having to rip and replace months and years of hard work and investments- too common in the data landscape.</li> <li>Evolving the organisation\u2019s data stack with complete ownership and control into higher-order frameworks (and innovations) instead of rupturing under the weight of rapid disruption.</li> </ul> <p>Modularisation is possible through a finite set of resources that have been uniquely identified as essential to the data ecosystem. These resources are distributed across the kernel layers to enable first, second, and multi-degree components that facilitate low-level operations and complex use-case-specific applications.</p> <p>The Data Developer Platform, technically defined, is a finite set of unique resources that talk to each other to declaratively enable any and every operation that data users, generators, or operators require- just like a common set of core building blocks or Lego pieces that can be put together to construct anything, be it a house, a car, or any other object. DDP core resources can similarly be put together to construct or deconstruct any data application that helps data developers serve and improve actual business value instead of investing their time and effort in the complex processes behind those outcomes.</p> <p>With the hierarchical design,  \u2192 DDP puts together stable subassemblies of fundamental first-order solutions (resources) \u2192 Builds second-order solutions on top of the common layer of atomic solutions (data products) \u2192 Continues to traverse n-times for n-order problems (mesh, fabric, data apps, etc.) </p> <p></p> <p>Therefore, when something faults in a DDP, every n-order solution only loses part of its capability, and the data developer:</p> <ul> <li>Does not need to relearn unique philosophies and languages for every higher-order solution</li> <li>Have a common lineage of (n-m)-order solutions to rollback on (m\u2265n)</li> <li>Does not need to undertake integration overheads given (n-m)-order solutions declaratively interoperate</li> <li>Rollout non-disruptive changes across all orders through dynamic configuration management</li> </ul> <p>Resources are atomic and logical units with their own life cycle, which can be composed together and also with other components and stacks to act as the building block of the Data Developer. Platform. They can be treated as artefacts that could be source-controlled and managed using a version control system. Every resource can be thought of as an abstraction that allows you to enumerate specific goals and outcomes in a declarative manner instead of the arduous process of defining \u2018how to reach those outcomes\u2019.</p> <p>DDP Resources or building blocks:</p> <ul> <li> <p>Workflow: Workflow is a manifestation of DAGs that streamlines and automates big data tasks. DDP uses it for managing both batch and streaming data ingestion &amp; processing.</p> </li> <li> <p>Service: Service is a long-running process that is receiving and/or serving APIs for real-time data, such as event processing and stock trades. It gathers, processes, and scrutinises streaming data for quick reactions.</p> </li> <li> <p>Policy: The policy regulates behaviour in a Data Operating System. Access policies control individuals, while data policies control the rules for data integrity, security, quality, and use during its lifecycle and state change.</p> </li> <li> <p>Depot: Depot offers a uniform way to connect to various data sources, simplifying the process by abstracting the various protocols and complexities of the source systems into a common taxonomy and route.</p> </li> <li> <p>Cluster: A cluster is a collection of computation resources and configurations on which you run data engineering, data science, and analytics workloads. A Cluster is provisioned for exploratory, querying, and ad-hoc analytics workloads.</p> </li> <li> <p>Secret: Secret store sensitive information like passwords, tokens, or keys. Users access it with a \u2018Secret\u2019 instead of using sensitive information. This lets you monitor and control access while reducing data leak risks.</p> </li> <li> <p>Database: Database resource is for the use cases where output is saved in a specific format. This resource can be used in all scenarios where you need to syndicate structured data. Once you create a Database, you can put a depot &amp; service on top of it to serve data instantly.</p> </li> <li> <p>Compute: Compute resources are the processing power required by any workflow/service or query workload to carry out tasks. Compute is related to common server components, such as CPUs and RAM. So a physical server within a cluster would be considered a compute resource, as it may have multiple CPUs and gigabytes of RAM.</p> </li> <li> <p>Operator: Operator registers resources that are not native to DDP and enables the orchestrator in the control to identify and orchestrate these resources. The operator resources make DDP truly extensible and customisable.</p> </li> <li> <p>Bundle: Bundle, as the name suggests, is a collection of several resources coming together to form an isolated branch in the DDP.  A bundle can define the instructions for enabling the specifications of a data product or a data application.</p> </li> </ul> <p> </p>"},{"location":"architecture/#extensible-architecture-workshop-for-higher-order-architectures","title":"Extensible Architecture: Workshop for Higher-Order Architectures","text":"<p>The flexible or composable infrastructure of the DDP allows it to become the foundation for any data design architecture an organisation chooses. The rearrangeable set of finite primitives enables users to declaratively trigger the necessary golden paths to interoperate and establish\u00a0complex architectures.</p> <p>DDP, therefore, can enable specific data design architectures with just a few degrees of rearrangement. Given it is also a self-service platform for data developers, including data engineers and data architects, setting up these frameworks is even more low-effort-high-impact. Architects don\u2019t have to reinvent the wheel and ideate from scratch. Instead, they already have their first-order capabilities, such as storage, compute, cluster, and others ready on an easily consumable plane. All they need to do is rearrange these blocks to enable their data strategies.</p>"},{"location":"data_product_specification_on_a_ddp/","title":"Data Product Specification on a DDP","text":"<p>TBD</p>"},{"location":"ddp_capabilities/","title":"Capabilites","text":""},{"location":"ddp_capabilities/#ddp-capabilities","title":"DDP Capabilities","text":""},{"location":"ddp_capabilities/#ddp-core-capabilities","title":"DDP Core Capabilities","text":"<ol> <li> <p>Configuration Management</p> <p>Advanced management of configurations for fine-tuning Data Products, allowing data developers to adjust settings and parameters dynamically.</p> </li> <li> <p>Infrastructure Orchestration</p> <p>Automating and optimising underlying infrastructure resources, such as computing and storage, to support the demanding data processing and analytics requirements.</p> </li> <li> <p>Environment Management</p> <p>Comprehensive management of development, testing, and production environments tailored to data workflows, ensuring controlled and efficient data product development.</p> </li> <li> <p>Deployment Management</p> <p>Streamlined packaging, deployment, versioning, and release management processes for Data Products, enabling efficient and consistent deployment across various environments.</p> </li> <li> <p>Attribute-Based Access Control</p> <p>Granular access control mechanisms, based on attributes, regulate permissions and ensure secure access to data resources within the Data Products.</p> </li> </ol>"},{"location":"ddp_capabilities/#capabilities-in-the-interest-of-data-personas","title":"Capabilities in the Interest of Data Personas","text":""},{"location":"ddp_capabilities/#data-developer","title":"Data Developer","text":"<p>Responsible for building, deploying, monitoring, and scaling Data Products, aka Data Engineers, AI/ML Engineers.</p> <ul> <li>Flexibility: DDP provides data developers with a flexible and modular environment, allowing them to choose the best tools, frameworks, and resources to build, deploy, and scale their data products.</li> <li>Modularity and Extensibility: DDP offers a set of standardised resources and design patterns that enable data developers to easily compose and extend their data solutions, promoting code reuse and accelerating development cycles.</li> <li>Streamlined Deployment and Monitoring: DDP automates and streamlines the deployment and monitoring processes, enabling data developers to quickly iterate on their data products and ensure their optimal performance and reliability.</li> <li>Declarative Freedom; Scalability: DDP provides the necessary infrastructure and tools to scale data products efficiently, handling increasing data volumes, processing demands, and user traffic. Data developer can declaratively build data pipelines and has more time to focus on spinning up applications and deploying them to produce tangible impact on the business ROI.</li> </ul>"},{"location":"ddp_capabilities/#operator","title":"Operator","text":"<p>Responsible for infrastructure orchestration, deployment management, and ensuring data governance and security within the platform</p> <ul> <li>Ecosystem Management: DDP facilitates the management of the data development ecosystem by providing centralized control and governance mechanisms. DDP operators can easily integrate and manage various tools, technologies, and services, ensuring seamless collaboration and interoperability.</li> <li>Simplified DevOps: DDP enables operators to implement streamlined DevOps practices specific to data development. It automates infrastructure provisioning, configuration management, and deployment processes, reducing operational complexities and enabling faster time-to-market.</li> <li>Security and Compliance: DDP includes robust security and compliance features, allowing DDP operators to enforce access controls, data protection measures, and privacy regulations. It ensures that data products and associated processes adhere to organisational and regulatory requirements.</li> </ul>"},{"location":"ddp_capabilities/#data-developers-in-action","title":"Data Developers in Action","text":"<p>Personas across DDP\u2019s three planes and their primary task(s) in each plane.</p> <p></p> DDP Plane Primary Persona Secondary Persona Tertiary Persona Control Plane Platform Engineer (Orchestration health) Operator (Governance) Data Engineer (Metadata insights) Development Plane Data Engineer (Declarative specs) Platform Engineer (Resource health, Extensibility) Operator (DevOps, Supervision) Data Activation Plane Data Engineer (App development) Data Producer &amp; Consumer (Data generation &amp; consumption) Operator (DevOps, Supervision)"},{"location":"ddp_capabilities/#workflow-data-product-development","title":"Workflow: Data Product Development","text":"<p>Baseline: The three planes are pre-connected in DDP, so data developers are not required to integrate the capabilities across these planes.</p> <ol> <li>Leverage DDP\u2019s config templates to bundle resources into an isolated instance of infrastructure for a data product.</li> <li>Declare deployment environment and variables.</li> <li>Create and publish policies as per the domain.</li> <li>Deploy the data product.</li> <li>Build one or more data applications that consume the data product.</li> </ol>"},{"location":"what_is_data_developer_platform/","title":"What","text":""},{"location":"what_is_data_developer_platform/#what-is-data-developer-platform","title":"What is Data Developer Platform","text":"<p>A Data Developer Platform is a Unified Infrastructure Specification to abstract complex and distributed subsystems and offer a consistent outcome-first experience to non-expert end users. A Data Developer Platform (DDP) can be thought of as an internal developer platform (IDP) for data engineers and data scientists. Just as an IDP provides a set of tools and services to help developers build and deploy applications more easily, a DDP provides a set of tools and services to help data professionals manage and analyze data more effectively.</p> <p>A DDP typically includes tools for data integration, processing, storage, and analysis, as well as governance and monitoring features to ensure that data is managed, compliant and secure. The platform may also provide a set of APIs and SDKs to enable developers to build custom applications and services on top of the platform. The primary value add of a DDP lies in its unification ability- instead of having to manage multiple integrations and overlapping features of those integrations, a data engineer can breathe easy with a clean and single point of management.</p> <p>In analogy to IDP, a DDP is designed to provide data professionals with a set of building blocks that they can use to build data products, services, and data applications more quickly and efficiently. By providing a unified and standardised platform for managing data, a data developer platform can help organizations make better use of their data assets and drive business value.</p> <p></p>"},{"location":"what_is_data_developer_platform/#guiding-principles-behind-ddp","title":"Guiding Principles Behind DDP","text":"<p>When you build a product, you build it with a purpose in mind. The Data Developer Platform specification was also similarly designed with a set of principles guiding development since ideation. The entire DDP spec sticks to these core guidelines:</p>"},{"location":"what_is_data_developer_platform/#data-product-first-or-product-thinking","title":"Data-Product-First or Product Thinking","text":"<p>To establish DDP, the first step is to develop a product mindset and understand that data itself is a product. Establishing the product mindset is, in fact, the largest barrier in the data economy. However, unlike prevalent practices, data must be managed as a product to leverage optimal business value and incur minimal loss.</p> <p>While there is no technology barrier to treating data as a product, there is also no unique tech that could claim to be its ideal implementation. This is because imbibing a product mindset in the data economy is more of a cultural challenge. However, DDP as a specification comes along with templatised processes and practices that guide data teams with the meandering journey of establishing the culture of product mindset and consequently materialising data products as a result within weeks instead of months and years.</p>"},{"location":"what_is_data_developer_platform/#data-stack-as-a-product","title":"Data Stack as a Product","text":"<p>Such a product mindset led to a unified architecture, which is the backbone of DDP. On the contrary, assembled systems were developed as a collection of sidecars, each one solving an ad-hoc requirement. Every sidecar or point tool came with its own product philosophy, semantics, and capabilities which, over time, clashed with the capabilities of other point tools. So when these tools were assembled together, the data stack was not a product but a messy web; quite far from the merits of a product approach. Any user who used the assembled stack tumbled into the dark trenches of duplicated and misdirected effort, overwhelming change management, and inadequate data operationalisation.</p> <p>On the other hand, unified systems approach the data stack as a product right from initiation. Any ad-hoc problem is managed not by adding clashing sidecars but by combining fundamental building blocks of the unified architecture, which have common philosophies and semantics and no overlapping capabilities. Moreover, extensibility is not compromised in this approach. Data developers can still bring in point tools, but when they plug them into the unified infra, their primary capability is identified and propagated across the data stack instead of these tools operating in silos.</p> <p>If we take the same example we used here, in assembled systems, two different catalogs and policy engines would be created, operating in silos within the compatible product philosophy of each tool. Whereas, in unified systems, the cataloguing tool would gain visibility of policies for discoverability in the governance tool, and the governance tool would be able to extend policies to the catalog items. This becomes possible due to an interoperable interface developed on top of the unified architecture, essentially eliminating dual catalogs and governance engines.</p>"},{"location":"what_is_data_developer_platform/#data-as-a-product","title":"Data as a Product","text":"<p>Data behaves like a product when it becomes discoverable, addressable, accessible, governable, easy to understand, and in sync with the quality requirements and goals of its consumers. If the data stack itself is a collection of ad-hoc overlapping tools put together to enable the above (governance, observability, discoverability, etc.), over time, the overlap of capabilities results in multiple catalogs, governance engines, discrepant monitors, and parallel efforts to maintain the promised data experience.</p> <p>Not surprisingly, this design breaks over time and the promise of experiencing data as a product is broken. Overlap of tools results in high technical debt, duplicate and corrupt data, incomprehensible metadata, and consequently multiple maintenance tickets dumped on unfortunate data engineering teams. There is practically no time to focus on the data itself because the leaky cauldron requires fixing before the stew can be perfected. Governance and observability become afterthoughts, and maintenance goes up on the priority list.</p> <p>To truly enable the experience of data as a product, these capabilities need to be embedded as part of the product itself instead of being added as sidecars. The unified architecture makes this feasible by collapsing conflicting capabilities, progressively eliminating debt, and enabling data developers to focus on data for a change instead of burning hours on plumbing. </p>"},{"location":"what_is_data_developer_platform/#software-principles-for-data","title":"Software Principles for Data","text":"<p>DDP is built as a programmable data platform which encapsulates the low-level data infrastructure and enables data developers to shift from build mode to operate mode with state-of-the-art developer experience. DDP achieves this through running infrastructure-as-code (IaC). Developers can create and deploy config files or new resources and declaratively provision, deploy, and manage them. For instance, provisioning RDS through IaC.</p> <p>Data developers could control data pipelines through a single workload specification or single window of change management, customize high-level abstractions on a need basis, use lean CLI integrations they are familiar with, and overall experience software-like robustness through importable code, prompting interfaces with smart recommendations, and on-the-fly debugging capabilities.</p> <p>DDP approaches data as software and wraps code systematically to enable object-oriented capabilities such as abstraction, encapsulation, modularity, inheritance, and polymorphism across all the components of the data stack. CRUD operations on data and data artefacts follow the software development lifecycle undergoing review cycles, continuous testing, quality assurance, post-deployment monitoring and end-to-end observability. This enables reusability and extensibility of infra artefacts and data assets, enabling the data product paradigm.</p> <p>Data as a product is a subset of the data-as-a-software paradigm wherein data inherently behaves like a product when managed like a software product, and the system is able to serve data products on the fly. This approach enables high clarity into downstream and upstream impact, along with the flexibility to zoom in and modify modular components.</p> <p></p>"},{"location":"what_is_data_developer_platform/#what-is-the-tangible-outcome-of-a-ddp","title":"What is the tangible outcome of a DDP","text":"<p>A data developer platform establishes a flexible infrastructure to materialise desired data solutions and a plethora of data design patterns on top of its infrastructure. These design patterns are essentially frameworks that sit on top of data orchestrated by the DDP. A DDP\u2019s job is to ensure the data is reliable and of the required data quality- enter Data Products.</p> <p>Data Product brings product thinking and software development principles to the data domain, allowing data experts to accelerate value creation and delivery. Data Product empowers developers by streamlining the creation process, eliminating duplication of assets and efforts, and enabling rapid iteration and experimentation. </p> <p></p> <p>A Data Developer Platform enabling the Data Product Construct</p> <p>The idea of \u201cData Products\u201d holds significant value since it enables the key pillars of obligatory data requirements: Discoverable, Addressable, Understandable, Native accessible, Trustworthy, Interoperable, Independent, and Secure.</p> Attribute Description Discoverable A Data Product must be easy to find. Addressable A Data Product must be easy to refer to across the data ecosystem. Understandable A Data Product must be easy to comprehend &amp; consume. Natively Accessible A Data Product must be easy to operate on with pre-existing tools and processes in the domain that users (domain citizens) are already comfortable with. Trustworthy A Data Product must guarantee the quality and authenticity of the data it presents. Interoperable A Data Product must be able to network with other data products and resources from across the data stack. Independent A Data Product must be self-contained and valuable on its own, and the data it presents should serve a purpose. Secure A Data Product must be guarded against bad actors, human or machine, throughout its journey in the data stack. <p>There are multiple ways to build a data product depending on the organisation\u2019s philosophy and approach to technology. One might choose to implement a data mesh pattern, while another might go for a data fabric. Some might discard either of the above to build a minimum viable data product pattern that works for their specific use cases. A DDP inherently furnishes a minimum viable pattern to enable the Data Product construct. Let's look at the definition of a data product to understand how DDP materialises it.</p> <p>A Data Product is a fundamental architectural unit of a data stack, and it has three components:</p> <ol> <li>Data &amp; Metadata</li> <li>Code or Instructions</li> <li>Infrastructure</li> </ol>"},{"location":"what_is_data_developer_platform/#what-is-a-data-product","title":"What is a Data Product?","text":"<p>A Data Product is a fundamental and independent unit of your data stack that encompasses all the resources, instructions, metadata and data to serve a specific business purpose.</p> <p>Data Product Components and How DDP enables each</p> <p></p> <p>To become an independent unit, a Data Product needs to wrap three structural components:</p>"},{"location":"what_is_data_developer_platform/#code-or-instructions","title":"Code or Instructions","text":"<p>The code required for data management is far from simple or straightforward. It encompasses data pipelines (ingestion, transformation, and serving), APIs (access, metadata, and metrics), and enforcement (policies, compliances, lineage &amp; provenance). The reason why the data ecosystem is not as robust as the software ecosystem is that code for data management was never approached from a software development angle. In prevalent data stacks, all these code components are isolated and are unable to talk to each other.</p> <p>\ud83d\udd11 We do not just suffer from data silos, but there\u2019s a much deeper problem. We also suffer from data code silos. </p> <p>DDP approaches data as software and wraps all code in one place to enable object-oriented capabilities such as abstraction, encapsulation, modularity, inheritance, and polymorphism across all the components of the data stack. Most importantly, this enables complete isolation and independence for the Data Product unit. The instructions not only run the data lifecycle but also orchestrate the lifecycles of different resources in the DDP infrastructure.</p> <p>Developers can experience state-of-the-art devX when it comes to code through standardised configurations, templates for scale, familiar CLI and DSL, coding prompts, importable libraries, and much more. In addition to that, becoming data-first within weeks becomes possible through the composable resources of DDP that are directed by the code in each Data Product as necessary.</p> <p></p>"},{"location":"what_is_data_developer_platform/#data-metadata","title":"Data &amp; Metadata","text":"<p>Data is undeniably powerless without metadata. The Data Product construct understands this and augments heterogeneous sources to tap into rich metadata. While each source and integration come with unique semantic specifications, DDP allows the data product to converge and maintain universal semantics.</p> <p>\ud83d\udd11 Universal semantics allows the data product to become addressable, discoverable, interoperable, and natively accessible. </p> <p>This makes it possible to identify associations between data assets, surface lineage, provenance, observability metrics, and key relations across the data ecosystem, be it a native DDP resource or a foreign third-party tool. It also enables the discovery of latent information, and with open APIs, users can augment and leverage both data and metadata programmatically. With access to rich metadata, data developers can throw more light on \u2018dark data\u2019 (rich yet dormant information limited by subpar semantics) and resurrect it from the trenches. </p> <p>DDP enables high-definition metadata through a central control plane that has complete visibility across the data ecosystem and is able to oversee metadata from heterogenous polyglot sources. Data contracts as one of the key resources in DDP, add life. to the metadata. A data contract is nothing but strongly typed metadata + enforcement. Implementing a contract inherently means establishing declarative enforcement. Contracts can either pull metadata from the central plane or be explicitly defined, and once established, they act as a guarantee on the specifications and make change management practically seamless.</p> <p></p>"},{"location":"what_is_data_developer_platform/#infrastructure","title":"Infrastructure","text":"<p>The infrastructure is the foundation on which data products are built and deployed. While having dedicated data product owners and platform developers becomes essential to produce and maintain data products in a regular setting, the DDP infrastructure frees the organisation or data teams from the additional friction of onboarding, training, and maintaining infrastructure or platform teams.</p> <p>The DDP Infrastructure is unified, self-serve, abstracts complex and distributed subsystems, omits excessive integration and maintenance overload, and encapsulates building blocks of a data product while maintaining customisation flexibility to offer a consistent outcome-first experience for both expert and non-expert end users.</p> <p>A unified Infrastructure, as one might assume at first glance, is not an exercise of tying together multiple tools with disparate capabilities together. This goes against the primary principle of having a unified foundation. Instead, a unified infra is built upon a common set of first-order solutions, or building blocks, that are composed together to build higher-order complex solutions. Such composability is made possible by building and running Infrastructure as Code.</p> <p>Examples of first-order solutions: Storage, Compute, Workflow, Policy, Volume, Cluster, etc.Examples of higher-order solutions: Security, Quality, Ingestion, Transformation, Streaming, etc. </p> <p></p> <p>The Unified Data Architecture was predicted and recommended way back in 2020, which is over three years ago. However, it largely did not get any immediate action or saw many aligned initiatives to bring the idea to fruition at scale. That has changed with the DDP specification, which embodies the unified architecture and delivers its promised benefits.</p> <p></p> <p>Components or Capabilities of a Unified Infrastructure | Courtesy of\u00a0a16z</p> <p>The unified infrastructure is future-proof in the sense that it is able to implement a wide range of design patterns, including the latest mesh, due to the availability of low-level building blocks.</p> <p>The fundamentals don\u2019t change nearly as fast as the latest tech de jour. Focus on what doesn\u2019t change and build from there. ~\u00a0Joe Reis, Author of Fundamentals of Data Engineering </p> <p></p>"},{"location":"what_is_data_developer_platform/#the-data-product-mvp-and-higher-order-solutions","title":"The Data Product MVP and Higher Order Solutions","text":"<p>A bare minimum DDP enables the infrastructure for data products, while a higher-order DDP comes with minimalistic templates for code and design frameworks. Plugging in data to a DDP allows it to capture and embed metadata and eventually serve data that checks off the requirements of a data product. DDP\u2019s unified infrastructure and data-first approach is inherently the minimal viable pattern for a data product.</p> <p>However, the story doesn\u2019t end at that. Every organisation comes with its own approach to data, and it might so happen that they are not happy with a base pattern. The good news is they don\u2019t have to start from scratch. Architects can easily build higher-order patterns on top of DDP resources, which are atomic non-negotiable units of any data stack, identified and embedded as part of the DDP and orchestrated through the unified architecture.</p> <p>Once an architect or data engineer has access to low-level resources, they can compose them together to manifest higher-order complex designs such as those of data mesh, data fabric, and even further higher-order patterns such as CDPs and customer 360s- all of which are designed and aimed at serving valuable data products when and where needed.</p>"},{"location":"what_is_data_developer_platform/#what-does-a-data-product-look-like-on-ddp","title":"What does a Data Product  look like on DDP","text":"<p>Broadly Data Product definition can be broken down into four aspects - Input, Transformation, SLOs and Output. Along with these, the user will have to create specs and makefile.</p> <p> DDP as implemented on DataOS (a data platform modelled after DDP principles)</p> <ol> <li> <p>Input</p> <p>The input aspect of a Data Product focuses on the technical mechanisms for data access or ingestion, such as APIs, streaming, connectors, and batch processes. These components enable the Data Product to acquire data from various sources, ensuring seamless and reliable data flow into the system for further processing and analysis. DDP Depots and APIs are supported mechanisms along with support for third-party tools like ADF, Fivetran etc.</p> </li> <li> <p>Transformation</p> <p>The transformation aspect of a Data Product involves the processing and manipulating of data within the product. This may include data cleansing, enrichment, aggregation, normalisation, or any other data transformations required to make the valuable data for analysis and consumption, which meets the desired format, structure, and quality. Transformation is defined using DDP Native-Stacks and Operators that integrate third-party tools like DBT, DB notebooks etc.</p> </li> <li> <p>SLOs </p> <p>SLOs define the performance, availability, accessibility, and quality targets a Data Product aims to achieve. These objectives ensure that the Data Product meets the required service levels regarding quality and governance. SLOs may include metrics defined on business, metadata and operations data.  Monitoring and managing SLOs help ensure that the Data Product performs optimally and meets the expectations of its consumers. SLOs are defined using DDP resources like quality, policy, alerts, monitors and metrics.</p> </li> <li> <p>Output</p> <p>The output aspect of a Data Product refers to the results or outcomes generated from the data analysis and processing. This includes the table, stream, APIs, visualisations, or web app delivered to the data consumers. DDP Depots, APIs, Streamlit Applications, JDBC ports etc., are supported. </p> </li> <li> <p>Spec File</p> <p>The spec file outlines technical details such as identification, domain, versioning, intent, objectives, and SLOs for the Data Product. It is a reference and guide for developing, deploying, and maintaining the product to ensure alignment with requirements.</p> </li> </ol>"},{"location":"what_is_data_developer_platform/#sample","title":"Sample","text":"<p>https://github.com/tmdc-io/dataproduct-sample</p>"},{"location":"what_is_data_developer_platform/#summarising-conceptual-philosophy-of-a-data-developer-platform","title":"Summarising: Conceptual Philosophy of a Data Developer Platform","text":"<p>A data developer platform\u2019s philosophy is a direct parallel of the Operating System philosophy. Yes, the idea which literally changed the world by giving encapsulated technology into the hands of the masses.</p> <p>While a gamer uses a Mac to run games, an accountant uses the same machine to process heavy Excel files. While a musician uses a phone to create complex media files, a grandparent uses it to video call the grandkids.</p> <p>Same platform. Different use cases.</p> <p>Same infrastructure. Variety of simple and complex solutions. </p> <p>In all cases, neither of the users needs to understand the low-level technology or build their applications from scratch to start using the applications for their desired outcomes. But does that mean there\u2019s no one on the other end figuring out the infrastructure complexities of the laptops, PCs, and phones?</p> <p>There indeed is a very small (compared to the size of the user base) team behind infrastructure building and maintenance, and their job is to ensure all the users on the other side have a seamless experience without the need to get into the nitty-gritty. If the infrastructure is well-kept, users are abstracted from the pains of booting, maintaining, and running the low-level nuances of day-to-day applications that directly bring them value.</p> <p>So is the case with a well-designed data developer platform. While smaller dedicated platform teams manage and regulate the infrastructure, larger teams of data developers are able to focus their time and effort on building data applications instead of worrying about plumbing issues. Applications across a broad range, including AI/ML, data sharing, and analytics, are all enabled at scale through the virtue of the same philosophy.</p> <p>With a DDP in place, the data developer is essentially abstracted from all the low-lying details- all now delegated to the unified infrastructure of the DDP. The only job of the data developer now is to build and enable data applications that directly power business value. And while the infrastructure takes care of the resources, environments, provisioning, and supervisory activities, a small dedicated platform team ensures that the infra is healthy and ready to do its job.</p>"},{"location":"why_ddp_for_data/","title":"Why","text":""},{"location":"why_ddp_for_data/#why-ddp-for-data","title":"Why DDP for Data","text":""},{"location":"why_ddp_for_data/#the-problem","title":"The Problem","text":"<p>The data world has been battling a persistent crisis, and it has only been exacerbated by the growing intensity of data owned and managed by organizations. Chaos has ensued for non-expert end users as data ecosystems progressively develop into complex and siloed systems with a continuous stream of point solutions added to the insane mix. Complex infrastructures requiring consistent maintenance\u00a0deflect most of the engineering talent from high-value operations, such as developing data applications that directly impact the business and ultimately enhance the ROI of data teams. Inflexible and unstable, and therefore, fragile data pipelines constrict data engineering teams as a\u00a0bottleneck for even simple data operations.\u00a0It is not uncommon to hear\u00a0a whole new data pipeline being spawned to answer one specific business question\u00a0or\u00a01000K data warehouse tables being created from 6K source tables.</p> <p>Data Consumers suffer from unreliable data quality, Data Producers suffer from duplicated efforts to produce data for ambiguous objectives, and Data Engineers suffer from flooding requests from both data production and consumption sides. The\u00a0dearth of exemplary developer experience\u00a0also robs data developers of the ability to declaratively manage resources, environments, and requests so they can focus completely on data solutions. Due to these diversions and the lack of a unified platform, it is nearly impossible for DEs to build short and crisp data-to-insight roadmaps. On top of that, it\u2019s a constant struggle to adhere to the organization\u2019s changing data compliance standards as\u00a0governance and observability become afterthoughts\u00a0in a maintenance-first setting. This directly impacts the quality and experience of data that passes through meandering pipelines blotched with miscellaneous integrations.</p> <p>Arriving at Assembled Data Stacks</p> <p>The concept of having an assembled architecture emerged over time to solve these common problems that infested the data community at large. One tool could tend to a particular problem, and assembling a collection of such tools would solve several issues. But, targeting patches of the problem led to a disconnected basket of solutions ending up with fragile data pipelines and dumping all data to a central lake that eventually created unmanageable data swamps across industries. This\u00a0augmented the problem by adding the cognitive load of a plethora of tooling\u00a0that had to be integrated and managed separately through expensive resources and experts.</p> <p>Data swamps are no better than physical files in the basement- clogged with rich, useful, yet dormant data that businesses are unable to operationalise due to disparate and untrustworthy semantics. Semantic untrustworthiness stems from a chaotic clutter of MDS, overwhelmed with tools, integrations, and unstable pipelines. Another level of semantics is required to understand the low-level semantics, complicating the problem further.</p> <p></p> <p>The MAD Ecosystem | Source: mattturck.com</p> <p>Two distinct features become more apparent with this kind of tooling overwhelm:</p>"},{"location":"why_ddp_for_data/#1-progressive-overlap-in-assembled-systems","title":"1. Progressive overlap in Assembled Systems","text":"<p>As more tools pop in, they increasingly develop the need to become independently operable, often based on user feedback. For instance, two different point tools, say one for cataloguing and another for governance, are plugged into your data stacks. This incites the need not just to learn the tools\u2019 different philosophies, integrate, and maintain each one from scratch but eventually pop up completely parallel tracks. The governance tool starts requiring a native catalog, and the cataloguing tool requires policies manageable within its system. Now consider the same problem at scale, beyond just two point solutions. Even if we consider the cost of these parallel tracks as secondary, it is essentially a significantly disruptive design flaw that keeps splitting the topology of one unique capability into unmanageable duplicates.</p>"},{"location":"why_ddp_for_data/#2-consistent-and-increasing-desire-to-decentralise","title":"2. Consistent and increasing desire to Decentralise","text":"<p>What follows from assembled systems is the sudden overwhelm of managing multiple limbs of the system, and therefore, increasing complexity and friction for end users to get their hands on the data. While business domains, such as marketing, sales, support, etc., have to jump multiple hops to achieve the data they need, the organisation feels the pressure to lift all dependencies clogging the central data team and distributing the workload across these domains. Ergo, it was not a surprise to see how the early Data Mesh laid urgent focus on domain ownership, or decentralisation in other words. While the idea seems very appealing on theoretical grounds, how feasible is it in the field? If we lay this idea on any working business model, there are a few consequences:c</p> <ul> <li>Not enough skilled professionals to allocate to each individual domain - Practically, how feasible is the idea of having data teams for each domain?</li> <li>Not enough professionals or budget to disrupt existing processes, detangle pipelines, and embed brand-new infrastructures.</li> <li>Not enough experts to help train and onboard during migration.</li> </ul> <p>It\u2019s both a skill- and resource-deficit issue. Moreover, with decades spent on evolving data stacks with not much value to show, organisations are not ideally inclined to pour in more investments and efforts to rip and replace their work. In essence, \ud835\udc00\ud835\udc2e\ud835\udc2d\ud835\udc28\ud835\udc27\ud835\udc28\ud835\udc26\ud835\udc32 instead should become the higher priority over Decentralisation if that is the ultimate objective.</p>"},{"location":"why_ddp_for_data/#the-solution","title":"The Solution","text":"<p>The true value of assembled systems cannot be neglected. Its biggest achievement has perhaps been the revolutionary shift to the cloud, which has made data not just more accessible but also recoverable.</p> <p>However, the requirement for unified over assembled systems was felt largely due to the overwhelming build-up of cruft within the bounds of data engineering. The more tools, the more overlap and cruft. **Traditional approaches often require specialised skills and resources, leading to bottlenecks and limited innovation.</p> <p>The\u00a0Data-First Stack (DFS)\u00a0is based on a unification approach or an umbrella solution that targets the weak fragments of the Traditional and Modern data stacks\u00a0as a whole instead of proponing their patchwork approach. A DDP, which is the technical embodiment of a Data-First Stack, democratises data product development by abstracting away low-level complexities and providing a unified platform that enables data professionals across the organisation to contribute and collaborate on data products. This democratisation fosters innovation, creativity, and a sense of ownership among data professionals.</p> <p></p>"},{"location":"why_ddp_for_data/#unified-architecture-over-assembled-architecture","title":"Unified Architecture over Assembled Architecture","text":"<p>DFS, as implemented by your organisation\u2019s data developer platform, brings together a curated set of self-service layers that eliminate redundant tools and processes to enable a reusable, modular, and composable operating platform, elevating user productivity. Now, instead of grinding to integrate and maintain hundreds of scattered solutions, users can put data first and focus on the core objectives: Building Data Applications that directly uplift business outcomes.</p> <p>The Data-First Stack is essentially a unified architecture design. The best parallel analogy for a unified architecture would be an operating system (OS). An OS is a program that manages all programs necessary for the end user to experience the service of outcome-driven programs instead of users figuring out \u2018how\u2019 to run those programs. Most of us have experienced OS on our laptops, phones, and, in fact, on most interface-driven devices. Users are hooked to these systems because they are abstracted from the pains of booting, maintaining, and running the low-level nuances of day-to-day applications that directly bring them value.</p> <p>\ud83d\udca1 The unified architecture of a Data-First Stack is materialised by a Data Developer Platform, enabling a self-serve data infra by abstracting users from the procedural complexities of applications and declaratively serving the outcomes.</p> <p>DDP serves as a unified platform that integrates various tools and technologies involved in the data product delivery process. It seamlessly integrates with popular developer tools, version control systems, container registries, orchestration frameworks, and infrastructure-as-code tools like Terraform. This integration simplifies the development workflow, enhances developer productivity, and enables the seamless adoption of new tools and technologies.</p> <p></p>"},{"location":"why_ddp_for_data/#a-closer-look-at-the-solution-achieving-the-state-of-data-products","title":"A Closer Look at the Solution: Achieving the State of Data Products","text":"<p>The sole objective of Data Developer Platforms is to create and enable Data Products for various operations and domains in a business. DDP\u2019s unified architecture has proven to be a fast and concrete path to establishing data products. The term \u201cData Products\u201d is perhaps new, but the concept it defines has persisted since businesses started working with data, that is since a caveman exchanged three bags of berries for one piece of steak. Why that is should be clear once we expand on the eight core attributes of data products. To briefly summarise, a data product enables reliable and high-quality data at scale to help accelerate business use cases.</p> Attribute Description Discoverable A Data Product must be easy to find. Addressable A Data Product must be easy to refer to across the data ecosystem. Understandable A Data Product must be easy to comprehend &amp; consume. Natively Accessible A Data Product must be easy to operate on with pre-existing tools and processes in the domain that users (domain citizens) are already comfortable with. Trustworthy A Data Product must guarantee the quality and authenticity of the data it presents. Interoperable A Data Product must be able to network with other data products and resources from across the data stack. Independent A Data Product must be self-contained and valuable on its own, and the data it presents should serve a purpose. Secure A Data Product must be guarded against bad actors, human or machine, throughout its journey in the data stack. <p>You can learn more about data products here.</p> <p>Businesses have desired each of these attributes in data since time immemorial, but only recently some of them have achieved this state like many other breakthroughs of the 21<sup>st</sup> century, especially around data including AI - the self-operating machine dependent on trustworthy data.</p> <p>We will soon find out how DDP bridges the gap between untrustworthy raw data and reliable Data Products, but before that, we need to see why the DDP specification has been a concrete approach so far. Platforms built with the DDP specification as a guideline have consistently delivered Data Products with full-proof testimonies from consumers of their data.</p>"},{"location":"why_ddp_for_data/#why-build-a-ddp-for-data-products","title":"Why build a DDP for Data Products","text":""},{"location":"why_ddp_for_data/#faster-time-to-data-products","title":"Faster Time to Data Products","text":"<p>Time and effort invested behind every deployment can vary based on the deployment load, team size, stage of organisation, expertise level, and much more. However, taking an average across hundred deployments, the time spent on specific tasks per deployment can be summarised as laid out by internaldeveloperplatform.org. These numbers can be replaced by calculating your own average stats per hundred deployments.</p> Procedure Frequency (%of deployments) Dev Time in hours (including waiting and errors) Ops Time in hours (including waiting and errors) Add/update app configurations (e.g. env variables) 5%* 1h* 1h* Add services and dependencies 1%* 16h* 8h* Add/update resources 0.38%* 8h* 24h* Refactor &amp; document architecture 0.28%* 40h* 8h* Waiting due to blocked environment 0,5%* 15h* 0h* Spinning up environment 0,33%* 24h* 24h* Onboarding devs, retrain &amp; swap teams 1%* 80h* 16h* Rollback failed deployment 1,75% 10* 20* Debugging, error tracing 4.40% 10* 10* Waiting for other teams 6.30% 16* 16* <p>*per 100 deployments</p>"},{"location":"why_ddp_for_data/#acceleration-by-abstraction","title":"Acceleration by Abstraction","text":"<p>These numbers do not necessarily matter when the number of deployments is low. However, when it comes to large organisations managing several projects with hundreds or even thousands of deployments every week, these numbers start hurting the ROI of data teams massively. Even adding just one tailing zero to the stats above shows us how existing maintenance processes suck out the valuable time of engineers. </p> <p>A Data Developer Platform is an easily operable, swift, self-served path to data products. With ready-to-deploy templates and standardised protocols, data developers can quickly spin up data applications without spending too much time on infrastructure dependencies and configurations. DDP enables higher deployment frequency for data products by abstracting most of the cognitive load and keeping the numbers from inflating drastically when the scale goes up.</p>"},{"location":"why_ddp_for_data/#acceleration-by-standardisation","title":"Acceleration by Standardisation","text":"<p>The API-first infrastructure of DDP enables modularisation and interoperability with both native and external components ensuring quicker deployments and rollback abilities without having to spend hours on infrastructure chores. Users can choose to process data at the source, push down queries and workloads, or even move processing to native environments if needed. With central management across development to deployment, monitoring, and maintenance, and seamless discovery of open APIs, users can augment and manipulate the data programmatically, version, release, configure, monitor, reproduce and much more to dramatically enhance the speed of data application development.</p> <p>DDP introduces standardised resources and design patterns, creating a shared language and best practices for data product development. This standardisation enhances collaboration among data developers, promotes code reusability, and reduces the risk of errors and inconsistencies in data products. Standardisation of configurations and templates plays a vital role in expediting the development of data products. Templates and preconfigured settings can be leveraged to expedite the creation of new data products, eliminating the need for repetitive and time-consuming setup and maintenance. </p> <p>Data developers can quickly deploy workloads by eliminating configuration drifts and vast number of config files through standard base configurations that do not require environment- or dependency-specific variables. The platform auto-generates manifest files for apps, enabling CRUD ops, execution, and meta storage on top.</p>   \ud83d\udca1 *In summary, DDP provides workload-centric development where data developers declare their requirements, and DDP takes care of provisioning and de-provisioning resources and resolving the dependencies. The impact is instantly realised with a visible increase in deployment frequency.*"},{"location":"why_ddp_for_data/#acceleration-by-modularisation","title":"Acceleration by Modularisation","text":"<p>But it is not just declarative infrastructure that enables faster time to data products. We also need to consider how adept the infrastructure is to enable the Data Products attributes: Discoverable, Addressable, Understandable, Natively Accessible, Trustworthy, Interoperable, Independent, and Secure. This also implies that governance and observability are no longer afterthoughts but are embedded in the data. By proactively addressing these aspects, organizations can enhance data quality, mitigate risks, and ensure compliance with regulatory requirements.</p> <p>DDP\u2019s modular infrastructure with fundamental atomic building blocks, where each block facilitates a unique capability on data, enables the above seamlessly. Decoupling services is a huge accelerator. And an even bigger accelerator is decoupling data applications from the data infrastructure. These two components are dangerously tied together in existing structures, leading to far more resources spent on tending to infra requirements from scratch for every application.</p> <p>Other than resource or capability modularisation, DDP also incorporates the modularisation of high-level solutions. For instance, DDP provides modularisation of governance policies with attribute-based access control (ABAC) mechanisms to ensure granular permissions regulation and secure data resource access. It enables fine-grained control over who can access, modify, and interact with data products, ensuring data privacy, compliance, and governance.</p> <p>The standard configurations enable a single point of change management, given it\u2019s built with reference to these atomic resources. The atomic units are able to easily communicate with each other to establish higher-order objectives such as, say, discoverability or policy management. Or even the standard data lifecycle processes such as data ingestion, processing, analysis, and visualisation - A DDP empowers data teams by providing a comprehensive platform that supports their end-to-end workflows. Data teams have the necessary tools, resources, and standardised practices to deliver high-quality data products, enabling them to make data-driven decisions and derive meaningful insights. You can read more on DDP building blocks here. </p>"},{"location":"why_ddp_for_data/#fluid-data-product-developer-experience","title":"Fluid Data Product Developer Experience","text":"<p>Data Products can be messy, given they encompass countless capabilities required around data. Governance policies, quality checks, clean cataloguing, accessible storage, transformation pipelines, orchestration of all the capabilities, and so much more. It is easy to understand how data engineers and data developers might feel trapped under thick layers of cognitive overload. Soon, the system and data bear the brunt of it. The data engineer\u2019s hands are always full of countless open tickets, rendering no space for innovation or strategy.</p> <p>DDP cuts down these issues by offering the following capabilities specifically designed for developer experience:</p>"},{"location":"why_ddp_for_data/#dynamic-configuration-management-dcm","title":"Dynamic Configuration Management (DCM)","text":"<p>Data Engineers are the victim of the current data ecosystem, as validated by a\u00a0recent study\u00a0which reports 97% of data engineers suffering from burnout. The prevalent data stacks compel data engineers to work repeatedly on fixing fragile fragments of countless data pipelines spawned at the rate of constant change, which is an inherent factor of data and data stacks. DCM enables unified management instead of having to tend to multiple environments, configuration files, and resources for one simple modification. DCM enables this through workload-centric development, where developers can describe and declaratively run their workloads through a single specification file that abstracts infrastructure complexities and dependencies.</p> <p></p>"},{"location":"why_ddp_for_data/#extensibility-and-customisability","title":"Extensibility and Customisability","text":"<p>DDP supports extensibility and customisation, allowing data developers to incorporate their own tools, libraries, and frameworks into the platform. It provides APIs and hooks for integrating custom functionalities, enabling data developers to tailor the platform to their specific needs and extend its capabilities. It seamlessly integrates with popular developer tools, version control systems, container registries, orchestration frameworks, and infrastructure-as-code tools like Terraform. Data developers can track the number of reusable components and libraries within the DDP ecosystem. Measure the percentage of data products that leverage these reusable components, aiming for a high adoption rate. Monitor the ease of integrating external tools and technologies, and measure the required reduction in the integration effort.</p>"},{"location":"why_ddp_for_data/#experimentation-friendly","title":"Experimentation-Friendly","text":"<p>By abstracting much of the cognitive overload, DDP opens up the ground for innovation, experiments, and mistakes - three key vitals of high-performance and high-ROI teams. A true data product cannot exist until a series of strategies have played out.  DDP not only surfaces virtualised layers as playgrounds but also shields the experimenter from the collateral damage of failed data product experiments. DDP takes the heat of innovation and also saves time spent on experimentation through smart capabilities, including intelligent data movement, semantic playgrounds, rollback abilities across pipelines and data assets, and declarative transformations where users can specify the inputs and outputs. DDP will automatically generate the necessary code to run the experiments. Data developers can declaratively prep, deploy, and de-provision clusters, provision databases and sidecar proxies, and manage secrets for lifetime with one-time credential updates- all through the simple and common syntax of DSL.</p>"},{"location":"why_ddp_for_data/#standardisation","title":"Standardisation","text":"<p>Developer experience is also significantly improved through the standardisation of key elements such as semantics, metrics, ontology, and taxonomy through consistent template prompts. The system auto-logs all operations and meta details to reduce process and pipeline overheads. Moreover, DDP comes with standardised templates for specifications and config files, drastically reducing the number of files that needs to be written and managed.</p>"},{"location":"why_ddp_for_data/#cli-first","title":"CLI-First","text":"<p>DDP enables a seamless experience for data developers by abstracting away repetitive and complex maintenance and integration overheads while allowing familiar CLI interfaces to run experiments, programmatically speak to data and resources, and build applications that directly impact business decisions and ROI. DDP becomes a complete self-service interface for developers where they can declaratively manage resources through APIs and the CLI.</p>"},{"location":"why_ddp_for_data/#self-service","title":"Self-Service","text":"<p>DDP enables\u00a0self-service for a broad band of data personas, including data engineers, business users, and domain teams. It allows a direct and asynchronous interface between data developers and data, eliminating complex layers of the infrastructure. Data developers can quickly spawn new applications and rapidly deploy them to multiple target environments or namespaces with configuration templates, abstracted credential management, and declarative workload specifications- all readily available for self-service. Containerised applications are consistently monitored and continuously tested for high uptime. In short, DDP brings together a curated set of self-service layers that eliminate redundant tools and processes to enable a reusable, modular, and composable operating platform, elevating user productivity.</p>"},{"location":"why_ddp_for_data/#fail-safe","title":"Fail-Safe","text":"<p>DDP is highly fault-tolerant with no central point of failure and flexibility to spin up hotfix environments, allowing for maximum uptime and high availability. Data Developers can create and modify namespaces in isolated clusters, create and execute manifest files, and independently provision storage, compute, and other DDP resources for self-service development environments. Data developers can rest easy with real-time insights into the health and status of data pipelines, workflows, and services, allowing for proactive monitoring, issue detection, and troubleshooting. Built-in monitoring and health management capabilities track data product performance, reliability, and availability.</p> <p>Real-time debugging in both dev and prod implementations enables rapid identification and resolution of issues before they become problems. Multi-hop rollbacks also provide an added layer of protection against unexpected issues, allowing for rapid restoration of previous configurations in the event of a problem. Finally, declarative governance and quality, in tandem with pre-defined SLAs, make DataOps a seamless and integrated part of the larger data management workflow and ensure that dataOps is no longer an afterthought but a core component of the data management approach.</p> <p></p>"},{"location":"why_ddp_for_data/#lower-cost-of-data-product-ownership","title":"Lower Cost of Data Product Ownership","text":"<p>The\u00a0total cost of\u00a0data product\u00a0ownership\u00a0is narrowed down enough to enable the development of multiple data products within the same band of available resources. Metrics specifically used in high-innovation and, consequently, high-performance environments, such as hours saved per data developer, cost of failed experiments and time to recovery, are all brought down to show visible business impact.</p> <p>Quantitatively, as we saw above, DDP significantly cuts down time and resources spent on infrastructure management, enabling teams to redirect their newfound excess to directly profitable verticals such as application development and innovative experiments. Resource optimisation is also one of the key USPs of DDP, given it optimises design on the level of resources to cut down duplication of pipelines for new or complex solutions.</p> <p>DDP provides mechanisms for the efficient scaling of data products. It allows for the seamless provisioning of resources, autoscaling of computing and storage, and load balancing to handle increasing data volumes and processing demands. This ensures optimised performance and scalability of data products, enabling them to handle large-scale data processing and analytics tasks.</p>"}]}